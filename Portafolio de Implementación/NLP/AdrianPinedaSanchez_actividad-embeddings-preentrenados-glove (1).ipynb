{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Using Pre-trained Word Embeddings\n","\n","In this notebook we will show some operations on pre-trained word embeddings to gain an intuition about them.\n","\n","We will be using the pre-trained GloVe embeddings that can be found in the [official website](https://nlp.stanford.edu/projects/glove/). In particular, we will use the file `glove.6B.300d.txt` contained in this [zip file](https://nlp.stanford.edu/data/glove.6B.zip).\n","\n","We will first load the GloVe embeddings using [Gensim](https://radimrehurek.com/gensim/). Specifically, we will use [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html)'s [`load_word2vec_format()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format) classmethod, which supports the original word2vec file format.\n","However, there is a difference in the file formats used by GloVe and word2vec, which is a header used by word2vec to indicate the number of embeddings and dimensions stored in the file. The file that stores the GloVe embeddings doesn't have this header, so we will have to address that when loading the embeddings.\n","\n","Loading the embeddings may take a little bit, so hang in there!"]},{"cell_type":"markdown","id":"11638210","metadata":{},"source":["## Adrian Pineda Sanchez A00834710"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["(400000, 300)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from gensim.models import KeyedVectors\n","\n","fname = \"C:/Users/adria/Downloads/glove.6B.300d.txt\"\n","glove = KeyedVectors.load_word2vec_format(fname, no_header=True)\n","glove.vectors.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Word similarity\n","\n","One attribute of word embeddings that makes them useful is the ability to compare them using cosine similarity to find how similar they are. [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) objects provide a method called [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) that we can use to find the closest words to a particular word of interest. By default, [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) returns the 10 most similar words, but this can be changed using the `topn` parameter.\n","\n","Below we test this function using a few different words."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:59.635513Z","iopub.status.busy":"2024-10-14T20:40:59.634994Z","iopub.status.idle":"2024-10-14T20:40:59.658035Z","shell.execute_reply":"2024-10-14T20:40:59.656573Z","shell.execute_reply.started":"2024-10-14T20:40:59.635444Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('cacti', 0.663456380367279),\n"," ('saguaro', 0.619585394859314),\n"," ('pear', 0.5233487486839294),\n"," ('cactuses', 0.5178282260894775),\n"," ('prickly', 0.5156316757202148),\n"," ('mesquite', 0.48448559641838074),\n"," ('opuntia', 0.45400843024253845),\n"," ('shrubs', 0.45362070202827454),\n"," ('peyote', 0.45344963669776917),\n"," ('succulents', 0.4512787461280823)]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# common noun\n","glove.most_similar(\"cactus\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:02.504028Z","iopub.status.busy":"2024-10-14T20:41:02.503614Z","iopub.status.idle":"2024-10-14T20:41:02.526066Z","shell.execute_reply":"2024-10-14T20:41:02.524663Z","shell.execute_reply.started":"2024-10-14T20:41:02.503989Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('cakes', 0.7506032586097717),\n"," ('chocolate', 0.6965583562850952),\n"," ('dessert', 0.6440261602401733),\n"," ('pie', 0.6087430119514465),\n"," ('cookies', 0.6082393527030945),\n"," ('frosting', 0.6017215251922607),\n"," ('bread', 0.5954801440238953),\n"," ('cookie', 0.5933820009231567),\n"," ('recipe', 0.5827102065086365),\n"," ('baked', 0.5819962620735168)]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# common noun\n","glove.most_similar(\"cake\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:54.137547Z","iopub.status.busy":"2024-10-14T20:40:54.137106Z","iopub.status.idle":"2024-10-14T20:40:54.159240Z","shell.execute_reply":"2024-10-14T20:40:54.157679Z","shell.execute_reply.started":"2024-10-14T20:40:54.137505Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('enraged', 0.7087872624397278),\n"," ('furious', 0.7078357338905334),\n"," ('irate', 0.6938743591308594),\n"," ('outraged', 0.6705202460289001),\n"," ('frustrated', 0.6515548229217529),\n"," ('angered', 0.6353201866149902),\n"," ('provoked', 0.5827428102493286),\n"," ('annoyed', 0.5818981528282166),\n"," ('incensed', 0.5751834511756897),\n"," ('indignant', 0.5704444646835327)]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# adjective\n","glove.most_similar(\"angry\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:07.278940Z","iopub.status.busy":"2024-10-14T20:41:07.278156Z","iopub.status.idle":"2024-10-14T20:41:07.303538Z","shell.execute_reply":"2024-10-14T20:41:07.301964Z","shell.execute_reply.started":"2024-10-14T20:41:07.278893Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('soon', 0.7661858797073364),\n"," ('rapidly', 0.7216639518737793),\n"," ('swiftly', 0.7197348475456238),\n"," ('eventually', 0.7043027281761169),\n"," ('finally', 0.6900883316993713),\n"," ('immediately', 0.684260904788971),\n"," ('then', 0.6697486042976379),\n"," ('slowly', 0.6645646095275879),\n"," ('gradually', 0.6401676535606384),\n"," ('when', 0.634766697883606)]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# adverb\n","glove.most_similar(\"quickly\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:08.868824Z","iopub.status.busy":"2024-10-14T20:41:08.867884Z","iopub.status.idle":"2024-10-14T20:41:08.891031Z","shell.execute_reply":"2024-10-14T20:41:08.889582Z","shell.execute_reply.started":"2024-10-14T20:41:08.868778Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('sides', 0.5867609977722168),\n"," ('both', 0.5843431949615479),\n"," ('two', 0.5652361512184143),\n"," ('differences', 0.5140715837478638),\n"," ('which', 0.5120178461074829),\n"," ('conflict', 0.5115456581115723),\n"," ('relationship', 0.5022750496864319),\n"," ('and', 0.49842509627342224),\n"," ('in', 0.4970666468143463),\n"," ('relations', 0.49701136350631714)]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# preposition\n","glove.most_similar(\"between\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["[('of', 0.7057957053184509),\n"," ('which', 0.6992015242576599),\n"," ('this', 0.6747024655342102),\n"," ('part', 0.6727458238601685),\n"," ('same', 0.6592391133308411),\n"," ('its', 0.6446542143821716),\n"," ('first', 0.6398990750312805),\n"," ('in', 0.6361347436904907),\n"," ('one', 0.6245333552360535),\n"," ('that', 0.6176422834396362)]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# determiner\n","glove.most_similar(\"the\")"]},{"cell_type":"markdown","metadata":{},"source":["## Word analogies\n","\n","Another characteristic of word embeddings is their ability to solve analogy problems.\n","The same [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method can be used for this task, by passing two lists of words:\n","a `positive` list with the words that should be added and a `negative` list with the words that should be subtracted. Using these arguments, the famous example $\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$ can be executed as follows:"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:47.775519Z","iopub.status.busy":"2024-10-14T20:40:47.775015Z","iopub.status.idle":"2024-10-14T20:40:47.799547Z","shell.execute_reply":"2024-10-14T20:40:47.797955Z","shell.execute_reply.started":"2024-10-14T20:40:47.775459Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('queen', 0.6713277101516724),\n"," ('princess', 0.5432625412940979),\n"," ('throne', 0.5386105179786682),\n"," ('monarch', 0.5347574353218079),\n"," ('daughter', 0.49802514910697937),\n"," ('mother', 0.49564430117607117),\n"," ('elizabeth', 0.483265221118927),\n"," ('kingdom', 0.47747087478637695),\n"," ('prince', 0.4668240249156952),\n"," ('wife', 0.4647327959537506)]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# king - man + woman\n","glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"]},{"cell_type":"markdown","metadata":{},"source":["Here are a few other interesting analogies:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["[('airplane', 0.5897148847579956),\n"," ('flying', 0.5675230026245117),\n"," ('plane', 0.53170245885849),\n"," ('flies', 0.5172374844551086),\n"," ('flown', 0.514790415763855),\n"," ('airplanes', 0.5091356635093689),\n"," ('flew', 0.5011662244796753),\n"," ('planes', 0.4970923364162445),\n"," ('aircraft', 0.4957723915576935),\n"," ('helicopter', 0.45859551429748535)]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# car - drive + fly\n","glove.most_similar(positive=[\"car\", \"fly\"], negative=[\"drive\"])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:15.417540Z","iopub.status.busy":"2024-10-14T20:41:15.417143Z","iopub.status.idle":"2024-10-14T20:41:15.440746Z","shell.execute_reply":"2024-10-14T20:41:15.439283Z","shell.execute_reply.started":"2024-10-14T20:41:15.417473Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('sydney', 0.6780861616134644),\n"," ('melbourne', 0.6499180197715759),\n"," ('australian', 0.5948832035064697),\n"," ('perth', 0.5828553438186646),\n"," ('canberra', 0.5610731840133667),\n"," ('brisbane', 0.55231112241745),\n"," ('zealand', 0.524011492729187),\n"," ('queensland', 0.5193883776664734),\n"," ('adelaide', 0.5027670860290527),\n"," ('london', 0.4644603729248047)]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# berlin - germany + australia\n","glove.most_similar(positive=[\"berlin\", \"australia\"], negative=[\"germany\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:38.755140Z","iopub.status.busy":"2024-10-14T20:40:38.754733Z","iopub.status.idle":"2024-10-14T20:40:38.779812Z","shell.execute_reply":"2024-10-14T20:40:38.778395Z","shell.execute_reply.started":"2024-10-14T20:40:38.755102Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('iraq', 0.5320571660995483),\n"," ('fallujah', 0.48340919613838196),\n"," ('iraqi', 0.47287359833717346),\n"," ('mosul', 0.46466362476348877),\n"," ('iraqis', 0.43555372953414917),\n"," ('najaf', 0.43527641892433167),\n"," ('baqouba', 0.4206319749355316),\n"," ('basra', 0.4190516471862793),\n"," ('samarra', 0.41253671050071716),\n"," ('saddam', 0.4079156517982483)]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# england - london + baghdad\n","glove.most_similar(positive=[\"england\", \"baghdad\"], negative=[\"london\"])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:42.639916Z","iopub.status.busy":"2024-10-14T20:40:42.638976Z","iopub.status.idle":"2024-10-14T20:40:42.662396Z","shell.execute_reply":"2024-10-14T20:40:42.660998Z","shell.execute_reply.started":"2024-10-14T20:40:42.639868Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('mexico', 0.5726831555366516),\n"," ('philippines', 0.5445370078086853),\n"," ('peru', 0.48382270336151123),\n"," ('venezuela', 0.48166725039482117),\n"," ('brazil', 0.46643102169036865),\n"," ('argentina', 0.45490506291389465),\n"," ('philippine', 0.44178417325019836),\n"," ('chile', 0.4396097958087921),\n"," ('colombia', 0.4386259913444519),\n"," ('thailand', 0.43396779894828796)]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# japan - yen + peso\n","glove.most_similar(positive=[\"japan\", \"peso\"], negative=[\"yen\"])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:41:21.585842Z","iopub.status.busy":"2024-10-14T20:41:21.585127Z","iopub.status.idle":"2024-10-14T20:41:21.609205Z","shell.execute_reply":"2024-10-14T20:41:21.607890Z","shell.execute_reply.started":"2024-10-14T20:41:21.585789Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('tallest', 0.5077418684959412),\n"," ('taller', 0.47616493701934814),\n"," ('height', 0.46000057458877563),\n"," ('metres', 0.4584785997867584),\n"," ('cm', 0.4521271884441376),\n"," ('meters', 0.44067251682281494),\n"," ('towering', 0.42784246802330017),\n"," ('centimeters', 0.42345425486564636),\n"," ('inches', 0.41745859384536743),\n"," ('erect', 0.4087313711643219)]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# best - good + tall\n","glove.most_similar(positive=[\"best\", \"tall\"], negative=[\"good\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Looking under the hood\n","\n","Now that we are more familiar with the [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method, it is time to implement its functionality ourselves.\n","But first, we need to take a look at the different parts of the [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) object that we will need.\n","Obviously, we will need the vectors themselves. They are stored in the `vectors` attribute."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(400000, 300)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["glove.vectors.shape"]},{"cell_type":"markdown","metadata":{},"source":["As we can see above, `vectors` is a 2-dimensional matrix with 400,000 rows and 300 columns.\n","Each row corresponds to a 300-dimensional word embedding. These embeddings are not normalized, but normalized embeddings can be obtained using the [`get_normed_vectors()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_normed_vectors) method."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["(400000, 300)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["normed_vectors = glove.get_normed_vectors()\n","normed_vectors.shape"]},{"cell_type":"markdown","metadata":{},"source":["Now we need to map the words in the vocabulary to rows in the `vectors` matrix, and vice versa.\n","The [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html) object has the attributes `index_to_key` and `key_to_index` which are a list of words and a dictionary of words to indices, respectively."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#glove.index_to_key"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#glove.key_to_index"]},{"cell_type":"markdown","metadata":{},"source":["## Word similarity from scratch\n","\n","Now we have everything we need to implement a `most_similar_words()` function that takes a word, the vector matrix, the `index_to_key` list, and the `key_to_index` dictionary. This function will return the 10 most similar words to the provided word, along with their similarity scores."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def cosine_similarity(vec1, vec2):\n","    \"\"\"Calcula la similitud coseno entre dos vectores.\"\"\"\n","    dot_product = np.dot(vec1, vec2)\n","    norm_vec1 = np.linalg.norm(vec1)\n","    norm_vec2 = np.linalg.norm(vec2)\n","    return dot_product / (norm_vec1 * norm_vec2)\n","\n","def most_similar_words(word, vectors, index_to_key, key_to_index, topn=10):\n","    \"\"\"Encuentra las palabras más similares a la palabra dada.\"\"\"\n","    # 1. Obtener el ID correspondiente a la palabra dada\n","    word_id = key_to_index.get(word)\n","    if word_id is None:\n","        raise ValueError(f\"La palabra '{word}' no está en el vocabulario.\")\n","    \n","    # 2. Obtener el embedding de la palabra\n","    word_vector = vectors[word_id]\n","    \n","    # 3. Calcular las similitudes con todas las palabras en el vocabulario\n","    similarities = []\n","    for idx, vec in enumerate(vectors):\n","        similarity = cosine_similarity(word_vector, vec)\n","        similarities.append((idx, similarity))\n","    \n","    # 4. Ordenar por puntaje de similitud en orden descendente\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n","    \n","    # 5. Excluir la palabra original de los resultados\n","    similarities = [sim for sim in similarities if sim[0] != word_id]\n","    \n","    # 6. Obtener los top n palabras más similares\n","    top_words = [(index_to_key[idx], sim) for idx, sim in similarities[:topn]]\n","    \n","    return top_words"]},{"cell_type":"markdown","metadata":{},"source":["Now let's try the same example that we used above: the most similar words to \"cactus\"."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T19:34:48.069448Z","iopub.status.busy":"2024-10-14T19:34:48.069146Z","iopub.status.idle":"2024-10-14T19:34:48.355878Z","shell.execute_reply":"2024-10-14T19:34:48.354644Z","shell.execute_reply.started":"2024-10-14T19:34:48.069415Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('cacti', 0.66345644),\n"," ('saguaro', 0.61958545),\n"," ('pear', 0.5233487),\n"," ('cactuses', 0.5178282),\n"," ('prickly', 0.5156319),\n"," ('mesquite', 0.48448554),\n"," ('opuntia', 0.45400843),\n"," ('shrubs', 0.45362064),\n"," ('peyote', 0.45344964),\n"," ('succulents', 0.45127875)]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["vectors = glove.get_normed_vectors()\n","index_to_key = glove.index_to_key\n","key_to_index = glove.key_to_index\n","most_similar_words(\"cactus\", vectors, index_to_key, key_to_index)"]},{"cell_type":"markdown","metadata":{},"source":["## Analogies from scratch\n","\n","The `most_similar_words()` function behaves as expected. Now let's implement a function to perform the analogy task. We will give it the very creative name `analogy`. This function will get two lists of words (one for positive words and one for negative words), just like the [`most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) method we discussed above."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import numpy as np\n","from numpy.linalg import norm\n","\n","def cosine_similarity(vec1, vec2):\n","    \"\"\"Calcula la similitud coseno entre dos vectores.\"\"\"\n","    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n","\n","def analogy(positive, negative, vectors, index_to_key, key_to_index, topn=10):\n","    \"\"\"\n","    Encuentra las palabras que completan la analogía.\n","    \n","    positive: Lista de palabras que son positivas en la analogía (ej. ['rey', 'mujer']).\n","    negative: Lista de palabras que son negativas en la analogía (ej. ['hombre']).\n","    vectors: Matriz de embeddings de palabras.\n","    index_to_key: Diccionario que convierte índices a palabras.\n","    key_to_index: Diccionario que convierte palabras a índices.\n","    topn: Número de resultados más similares que quieres devolver.\n","    \"\"\"\n","    \n","    # Obtener los IDs de las palabras positivas y negativas\n","    pos_ids = [key_to_index[word] for word in positive]\n","    neg_ids = [key_to_index[word] for word in negative]\n","    \n","    # Obtener los embeddings de las palabras positivas y negativas\n","    pos_emb = np.sum([vectors[id] for id in pos_ids], axis=0)\n","    neg_emb = np.sum([vectors[id] for id in neg_ids], axis=0)\n","    \n","    # Obtener el vector de la analogía (positivos - negativos)\n","    emb = pos_emb - neg_emb\n","    \n","    # Normalizar el vector de la analogía\n","    emb = emb / norm(emb)\n","    \n","    # Calcular las similitudes con todas las palabras en el vocabulario\n","    similarities = []\n","    for idx, vec in enumerate(vectors):\n","        similarity = cosine_similarity(emb, vec)\n","        similarities.append((idx, similarity))\n","    \n","    # Ordenar las palabras por similitud en orden descendente\n","    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n","    \n","    # Excluir las palabras que se usaron en la analogía (positivas y negativas)\n","    given_word_ids = pos_ids + neg_ids\n","    similarities = [sim for sim in similarities if sim[0] not in given_word_ids]\n","    \n","    # Obtener los topn palabras más similares\n","    top_words = [(index_to_key[idx], sim) for idx, sim in similarities[:topn]]\n","    \n","    return top_words"]},{"cell_type":"markdown","metadata":{},"source":["Let's try this function with the $\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$ example we discussed above."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T20:40:30.924531Z","iopub.status.busy":"2024-10-14T20:40:30.923985Z","iopub.status.idle":"2024-10-14T20:40:30.961378Z","shell.execute_reply":"2024-10-14T20:40:30.960104Z","shell.execute_reply.started":"2024-10-14T20:40:30.924439Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[('queen', 0.6713277),\n"," ('princess', 0.5432625),\n"," ('throne', 0.5386105),\n"," ('monarch', 0.53475755),\n"," ('daughter', 0.49802515),\n"," ('mother', 0.4956443),\n"," ('elizabeth', 0.48326525),\n"," ('kingdom', 0.4774709),\n"," ('prince', 0.46682402),\n"," ('wife', 0.4647327)]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["positive = [\"king\", \"woman\"]\n","negative = [\"man\"]\n","vectors = glove.get_normed_vectors()\n","index_to_key = glove.index_to_key\n","key_to_index = glove.key_to_index\n","analogy(positive, negative, vectors, index_to_key, key_to_index)"]},{"cell_type":"markdown","id":"54116325","metadata":{},"source":["## **Conclusion**\n","\n","Como podemos ver en la lista anterior en word analogies donde utilizamos los mismos positive y negative atribuidos a \"king\" y \"woman\" en positive, asi como \"man\" en negative podemos observar que hemos implementado la funcion \"analogy\"  de forma correcta asi similar a la funcion de most_similar donde si observamos ambas listas de glove con las probabilidades obtenidas podemos observar que son las mismas.\n","\n","Probabilidades respecto a glove del ejemplo anterior para la prueba de la funcion \"analogy\":\n","\n","**king - man + woman**\n","\n","**glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])**\n","\n","- **queen**: 0.6713277101516724\n","- **princess**: 0.5432625412940979\n","- **throne**: 0.5386105179786682\n","- **monarch**: 0.5347574353218079\n","- **daughter**: 0.49802514910697937\n","- **mother**: 0.49564430117607117\n","- **elizabeth**: 0.483265221118927\n","- **kingdom**: 0.47747087478637695\n","- **prince**: 0.4668240249156952\n","- **wife**: 0.4647327959537506\n","\n","\n","Y podemos observar que tanto las palabras asi como los valores son equivalentes en el ejemplo utilizado, que es incluso el mismo que vimos y observamos en la clase, por lo tanto funciona."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":5}
